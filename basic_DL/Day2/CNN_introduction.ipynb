{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd /content/drive/MyDrive/LG_AI_2021/LG-Vision-Academy/basic_DL/Day2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data analysis\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Introduction with MNIST dataset\n",
    "\n",
    "본 자료는 CNN 모델에 대한 이해를 돕기 위해 제작된 사전 자료입니다.   \n",
    "본 학습 전에 `Pytorch` 패키지를 활용해 학습 데이터 구성과 컨볼루션 신경망 학습을 어떻게 하는지 알아보려고 합니다.    \n",
    "특히, MNIST 손글씨 데이터셋을 활용해 신경망 학습을 공부해보겠습니다.\n",
    "\n",
    "본 자료는 아래 분들의 자료를 참고하여 만들었습니다.   \n",
    "Copyright @ cb_park@korea.ac.kr (Cheonbok Park), joonleesky@kaist.ac.kr (Hojoon Lee)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성능 평가를 위한 Confusion Matrix(오차행렬)\n",
    "\n",
    "![image.png](https://t1.daumcdn.net/cfile/tistory/9908B7445E8BD68332)\n",
    "\n",
    "먼저 표 내부에 있는 단어들에 대한 설명을 드리겠습니다.   \n",
    "설명의 편의성을 위해서 환자다(Positive) / 환자가 아니다(Negative) 로 설명드리겠습니다.\n",
    "\n",
    "- Actual(Positive) : 실제 환자   \n",
    "- Actual(Negative) : 실제 환자가 아님   \n",
    "- Predict(Positive) : 실제 환자로 예측   \n",
    "- Predict(Negative) : 환자가 아닌 것으로 예측   \n",
    "\n",
    "이번엔 각각의 교차된 지표에 대해서 설명드리겠습니다. 이 부분은 앞의 단어를 **예측에 대한 성공/실패**, 뒤의 단어를 **긍정예측/부정예측** 이라고 생각하시면 생각하시기 쉬울겁니다!\n",
    "\n",
    "- TP(True Positive) : 긍정예측을 성공 즉, 환자라고 예측해서 실제 환자임을 맞춤   \n",
    "- TN(True Negative) : 부정예측을 성공 즉, 비환자라고 예측하여 실제 비환자임을 맞춤   \n",
    "- FP(False Positive) : 긍정예측을 실패 즉, 환자라고 예측했지만 비환자임   \n",
    "- FN(False Negative) : 부정예측을 실패 즉, 비환자라고 예측했지만 실제 환자임   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, target_names=None, cmap=None, normalize=True, labels=True, title='Confusion matrix'):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    \n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "    \n",
    "    if labels:\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            if normalize:\n",
    "                plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "            else:\n",
    "                plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader 생성\n",
    "\n",
    "MNIST 데이터의 경우 `Pytorch`의 `torchvision`에서 제공해주는 데이터셋입니다.   \n",
    "따라서, 해당 패키지로부터 데이터를 불러오고, SGD 경사하강법을 위해 `batch size`를 설정해주고 `transform` 함수를 통해 DataLoader로 아래의 코드처럼 만들어줍니다.\n",
    "\n",
    "더욱 자세한 설명은 https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader 을 참고하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can load various Pretrained Model from this package \n",
    "import torchvision\n",
    "import torchvision.datasets as vision_dsets\n",
    "# Transformation functions to manipulate images\n",
    "import torchvision.transforms as T\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoader(root='./data', train =True, transforms=None, download=True, batch_size=32, num_worker=1):\n",
    "    print (\"[+] Get the MNIST DATA\")\n",
    "    \"\"\"\n",
    "    We will use Mnist data for our tutorial \n",
    "    \"\"\"\n",
    "    mnist_train = vision_dsets.???(root = root,  #root is the place to store your data. \n",
    "                                    train = ???,  \n",
    "                                    transform = T.ToTensor(), # convert data to tensor \n",
    "                                    download = True)  # whether to download the data\n",
    "    mnist_test = vision_dsets.???(root = root,\n",
    "                                    train = ???, \n",
    "                                    transform = T.ToTensor(),\n",
    "                                    download = True)\n",
    "    \"\"\"\n",
    "    Data Loader is a iterator that fetches the data with the number of desired batch size. \n",
    "    * Practical Guide : What is the optimal batch size? \n",
    "      - Usually use a multiple of 16\n",
    "    \"\"\"\n",
    "    trainloader = ???(dataset = ???,  # information about your data type\n",
    "                      batch_size = ???, # batch size\n",
    "                      shuffle =???, # Whether to shuffle your data for every epoch. (Very important for training performance)\n",
    "                      num_workers = 1) # number of workers to load your data. (usually number of cpu cores)\n",
    "\n",
    "    testloader = ???(dataset = ???, \n",
    "                    batch_size = ???,\n",
    "                    shuffle = ???, # we don't actually need to shuffle data for test\n",
    "                    num_workers = 1) \n",
    "    \n",
    "    print (\"[+] Finished loading data & Preprocessing\")\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainloader, testloader = DataLoader(batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터셋을 시각화하며 어떤 특성을 가진 데이터인지 살펴보겠습니다.\n",
    "\n",
    "[MNIST](https://ko.wikipedia.org/wiki/MNIST_데이터베이스)는 손으로 쓴 숫자들로 이루어진 데이터입니다.   \n",
    "\n",
    "![mnist.png](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/220px-MnistExamples.png)\n",
    "\n",
    "위의 그림처럼, MNIST는 각 이미지가 0에서 9까지의 숫자 중 어떤 클래스에 속하는지를 학습하는 분류 문제에 사용되는 데이터셋입니다.    \n",
    "총 6만개의 학습 이미지와 1만개의 평가 이미지로 구성되어 있으며, 28x28 픽셀 크기의 흑백 그림으로 이루어져있습니다. \n",
    "\n",
    "`matplotlib` 패키지를 활용해, 데이터를 시각해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (image, label) in enumerate(trainloader):\n",
    "    print('image shape:', image.shape)\n",
    "    print('label shape:', label.shape)\n",
    "    \n",
    "    index = 0\n",
    "    plt.imshow(image[index].reshape(28, 28), 'gray')\n",
    "    print('Sample label:', label[index])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer 정의\n",
    "\n",
    "구현의 편의성을 위해 `Trainer`를 클래스로 만들어, 내부에 학습을 위한 `train`과 `test` 함수를 만드려고 합니다.     \n",
    "학습과 평가 두가지를 위해 모델, 데이터셋, 옵티마이저, 손실 함수 모두를 argument로 받으려고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, trainloader, testloader, model, optimizer, criterion):\n",
    "        \"\"\"\n",
    "        trainloader: train data's loader\n",
    "        testloader: test data's loader\n",
    "        model: model to train\n",
    "        optimizer: optimizer to update your model\n",
    "        criterion: loss function\n",
    "        \"\"\"\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def train(self, epoch):\n",
    "        \"\"\"\n",
    "        epoch: number of times each training sample is used\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        for e in range(epoch):\n",
    "            running_loss = 0.0  \n",
    "            for i, (inputs, labels) in enumerate(self.trainloader, 0): \n",
    "                # search on google \"upload input and label to gpu pytorch\"\n",
    "                inputs = inputs.???\n",
    "                labels = labels.???\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                ???\n",
    "                #  Q1) waht if we dind't clear up the gradients?\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = ??? # get output after passing through the network\n",
    "                \n",
    "                loss = ??? # compute model's score using the loss function \n",
    "                \n",
    "                ??? # perform back-propagation from the loss\n",
    "                ??? # perform gradient descent with given optimizer\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if (i+1) % 500 == 0:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' % (e + 1, i + 1, running_loss / 500))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "        print('Finished Training')\n",
    "        \n",
    "    def test(self):\n",
    "        self.model.eval() \n",
    "        # Q2) Why should we change the network into eval-mode?\n",
    "        \n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.???\n",
    "            labels = labels.???\n",
    "            \n",
    "            output  =???\n",
    "            \n",
    "            pred = torch.max(???, dim=???)[???]\n",
    "            correct += (??? == ???).sum().item()\n",
    "\n",
    "            test_loss /= len(self.testloader.dataset)\n",
    "            \n",
    "        print('\\nTest set:  Accuracy: {}/{} ({:.0f}%)\\n'.\n",
    "                format(correct, len(self.testloader.dataset),\n",
    "                100.* correct / len(self.testloader.dataset)))\n",
    "        \n",
    "    def get_confusion(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        confusion = torch.zeros(10,10)\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.???\n",
    "            labels = labels.???\n",
    "            \n",
    "            output = self.model(inputs)\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            \n",
    "            for num in range(output.shape[0]):\n",
    "                confusion[pred[num], labels[num]] += 1\n",
    "        \n",
    "        return confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, Optimizer, Criterion 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn #\n",
    "import torch.nn.functional as F # various activation functions for model\n",
    "import torch.optim as optim # various optimization functions for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 2-Layer FC model + Sigmoid\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: 30\n",
    "- Output dimension: 10\n",
    "- activation: sigmoid\n",
    "- Optimizer: SGD with lr=0.001\n",
    "- Loss: Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(???): \n",
    "    def __init__(self):\n",
    "        super(???, self).???()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = ???\n",
    "        self.fc1 = ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.view convert the shape of tensor, (Batch_size,28,28) --> (Batch_size,28*28)\n",
    "        x = ???\n",
    "        \n",
    "        x = self.???(x) # 28*28 -> 30 \n",
    "        x = ???(x) # Sigmoid Activation function (use F)\n",
    "        x = self.???(x)  # 30 -> 10, logit for each class\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().??? # create the neural network instance and load to the cuda memory.\n",
    "criterion = ??? # Define Loss Function. We use Cross-Entropy loss.\n",
    "optimizer = ??? # optimizer receives training parameters and learning rate.\n",
    "\n",
    "trainer = Trainer(trainloader = trainloader,\n",
    "                  testloader = testloader,\n",
    "                  model = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(trainer.get_confusion().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 2-Layer FC model + ReLU\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: 30\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: SGD with lr=0.001\n",
    "- Loss: Cross-Entropy\n",
    "\n",
    "미분값이 계속해서 곱해지는 역전파 방식에서 Sigmoid 함수는 양끝에서 0 근처의 미분값을 뱉기 때문에, vanishing gradient 문제를 일으킬 가능성이 큽니다.    \n",
    "따라서, 양수값에 대해서 1의 미분값을 뱉는 ReLU를 사용했을 때, 성능 향상을 기대해볼 수 있습니다.\n",
    "\n",
    "![relu.png](https://cdn-images-1.medium.com/max/357/1*oePAhrm74RNnNEolprmTaQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = ???\n",
    "        self.fc1 = ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = ???\n",
    "        x = self.???(x) # 28*28 -> 30 \n",
    "        x = ???(x) # ReLU Activation function\n",
    "        x = self.???(x)  # 30 -> 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().???\n",
    "criterion = ???\n",
    "optimizer = ???\n",
    "\n",
    "trainer = Trainer(trainloader = trainloader,\n",
    "                  testloader = testloader,\n",
    "                  model = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 3-Layer FC model + ReLU\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (50, 30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: SGD with lr=0.001\n",
    "- Loss: Cross-Entropy\n",
    "\n",
    "층이 깊어졌을 때 성능이 무조건 증가할지 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc0 = ??? # Layer 1\n",
    "        self.fc1 = ??? # Layer 2\n",
    "        self.fc2 = ??? # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = ???\n",
    "        x = self.???(x)\n",
    "        x = ???(x)\n",
    "        x = self.???(x)\n",
    "        x = ???(x)\n",
    "        x = self.???(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().??? \n",
    "criterion = ???\n",
    "optimizer = ???\n",
    "\n",
    "trainer = Trainer(trainloader = trainloader,\n",
    "                  testloader = testloader,\n",
    "                  model = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 2-Layer FC model + ReLU + Adam\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: 30\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- Optimizer: Adam with lr=0.001\n",
    "- Loss: Cross-Entropy\n",
    "\n",
    "이번엔 SGD가 아닌, 좀 더 최신의 Adam 옵티마이저를 사용해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = ???\n",
    "        self.fc1 = ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = ???\n",
    "        x = self.???(x) # 28*28 -> 30 \n",
    "        x = ???(x) \n",
    "        x = self.???(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().???\n",
    "criterion = ???\n",
    "optimizer = ???\n",
    "\n",
    "trainer = Trainer(trainloader = trainloader,\n",
    "                  testloader = testloader,\n",
    "                  model = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 2-Layer FC model + ReLU + Adam + Batch Normalization(BN)\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Hidden dimension: (30)\n",
    "- Output dimension: 10\n",
    "- activation: relu\n",
    "- normalization: BN\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy\n",
    "\n",
    "BN을 통해 각 배치 별로 평균과 분산을 이용해 정규화를 진행하는 것으로, covariate shift 문제를 해결해준다고 알려져 있습니다.\n",
    "\n",
    "![BN.png](https://cerebras.net/wp-content/uploads/2021/03/1ETvcPhYH1lCfXndMiKW-jQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = ???\n",
    "        self.bn = ??? # BatchNorm \n",
    "        self.fc1 = ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = ???\n",
    "        x = self.???(x) # 28*28 -> 30 \n",
    "        x = self.???(x) # BN\n",
    "        x = ???(x) \n",
    "        x = self.???(x)   \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().???\n",
    "criterion = ???\n",
    "optimizer = ???\n",
    "\n",
    "trainer = Trainer(trainloader = trainloader,\n",
    "                  testloader = testloader,\n",
    "                  model = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `model.train()` 혹은 `model.eval()` 함수는 BN 층처럼 학습과 평가때 동작이 다르게되는 층을 위해서 사용하는 함수입니다.\n",
    "\n",
    "아래 보시는 것처럼 BN을 사용함으로써 `running_mean`과 `running_var` 변수들이 생겼고, BN의 동작구조상 학습 때 구해진 평균과 분산을 평가때 사용되기 때문에 모델에게 학습 단계인지 아닌지를 알려주는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected 모델의 문제점은 파라미터의 수가 너무 많다는 것입니다.\n",
    "\n",
    "파라미터의 수가 많으면 연산 속도나 GPU 메모리도 문제지만, 편향과 분산 오류 관점에서 오버피팅 문제를 야기할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(mnist_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 2-Layer (Conv+FC) model + ReLU + Adam + BN\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Conv: 8 convolution filters with kernel_size=6 and stride=2 \n",
    "- Hidden dimension (FC): 8 * 12 * 12\n",
    "- Output dimension (FC): 10\n",
    "- activation: relu\n",
    "- normalization: batch-norm\n",
    "- Optimizer: Adam with lr=0.001\n",
    "- Loss: Cross-Entropy\n",
    "\n",
    "Convolution layer를 통해 이미지의 공간적인 정보(Height, Width)를 고려해 연산을 했을 때, 이미지 분류 태스크의 성능이 어떻게 변화하는지 살펴봅시다.\n",
    "![conv.png](https://miro.medium.com/max/2000/1*vkQ0hXDaQv57sALXAJquxA.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.conv = ???(in_channels = ???,\n",
    "                       out_channels = ???,\n",
    "                       kernel_size = ???,\n",
    "                       stride = ???) # Layer 1\n",
    "        self.bn = ???  # 2d batch-norm is used in 3d inputs\n",
    "        self.fc = ???   # Layer 2 \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.???(x)\n",
    "        x = self.???(x)\n",
    "        x = ???(x)\n",
    "        x = ???\n",
    "        x = self.???(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().???\n",
    "criterion = ???\n",
    "optimizer = ???\n",
    "\n",
    "trainer = Trainer(trainloader = trainloader,\n",
    "                  testloader = testloader,\n",
    "                  model = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(mnist_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 3-Layer (Conv+AvgPool+FC) model + ReLU + Adam + BN\n",
    "\n",
    "- Input: (28 * 28)\n",
    "- Conv: 8 convolution filters with kernel_size=6 and stride=2 \n",
    "- Pool: AvgPool with kernel_size=2 and stride=2\n",
    "- Hidden dimension (FC): 8 * 6 * 6\n",
    "- Output dimension (FC): 10\n",
    "- activation: relu\n",
    "- normalization: batch-norm\n",
    "- Optimizer: Adam\n",
    "- Loss: Cross-Entropy\n",
    "\n",
    "AvgPool을 통해 FC layer에 들어가기 전 공간적인 정보를 유지한채 크기를 줄여보려 합니다.     \n",
    "FC 층의 파라미터 수가 더 줄어들었을 때, 성능이 어떻게 변화할지 살펴봅시다.\n",
    "![pool.png](https://blog.kakaocdn.net/dn/biczZ1/btqED1eyDLf/c5cBddY5vw8DRo0K0S49L1/img.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.conv = ???(in_channels = ???,\n",
    "                       out_channels = ???,\n",
    "                       kernel_size = ???,\n",
    "                       stride = ???) # Layer 1\n",
    "        self.bn = ???\n",
    "        self.pool = ???\n",
    "        self.fc = ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.???(x)\n",
    "        x = self.???(x)\n",
    "        x = ???(x)\n",
    "        x = self.???(x)\n",
    "        x = ???\n",
    "        x = self.???(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().???\n",
    "criterion = ??? \n",
    "optimizer = ???\n",
    "\n",
    "trainer = Trainer(trainloader = trainloader,\n",
    "                  testloader = testloader,\n",
    "                  model = mnist_net,\n",
    "                  criterion = criterion,\n",
    "                  optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epoch = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(mnist_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
