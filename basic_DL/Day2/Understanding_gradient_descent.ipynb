{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd /content/drive/Mydrive/LG_AI_2021/LG-Vision-Academy/basic_DL/Day2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV8fQbEjDdHT"
   },
   "outputs": [],
   "source": [
    "# Basic setting\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKQgClCjDdHY"
   },
   "source": [
    "# Understanding Gradient Descent\n",
    "\n",
    "본 자료는 딥러닝 모델을 학습시키는 원리인 경사하강법(gradient descent)과 역전파(backpropagation)에 대한 이해를 목적으로 만든 자료입니다.    \n",
    "먼저 가장 단순한 선형 회귀 모델에 대한 경사하강법을 `numpy`와 `torch` 패키지를 활용해 구현해보려고 합니다.   \n",
    "\n",
    "그 뒤, 로지스틱 회귀 모델을 `numpy` 패키지로 학습하며 역전파의 개념에 대해 상기해보려 합니다.      \n",
    "또한, 딥러닝 패키지 중 하나인 `torch`를 통해 구현했을 때는 어떤 점이 다른지 살펴볼 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단순 선형 회귀 모델 (Regression 문제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwy3XfskDdHZ"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('./data/linear_regression.pickle', 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3Ojzp3_DdHa",
    "outputId": "884e5daa-4a69-4e6a-ce3f-79825ac7070e"
   },
   "outputs": [],
   "source": [
    "# Check the dataset distribution    \n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eExRk7g1DdHb"
   },
   "source": [
    "## [P1] `Numpy` 패키지를 활용해 최적의 선형 회귀 모델을 찾아봅시다.\n",
    "\n",
    "최적의 모델이란, 모든 데이터에 대한 예측값과 실제값의 차이를 가장 적게 만들어주는 최적의 파라미터를 가진 모델입니다.      \n",
    "예시로 주어진 회귀 문제를 해결하기 위해선, 일반적으로 MSE(Mean Squared Error) 손실 함수를 사용해 모델을 학습합니다.\n",
    "\n",
    "**MSE loss**\n",
    "\\begin{equation*}\n",
    "\\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - (wX_i + b))^2 \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "**Gradients calculation**\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial w} = -2 * \\frac{1}{n} \\sum_{i=1}^n (y_i - (wX_i + b)) * X_i\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial b} = -2 * \\frac{1}{n} \\sum_{i=1}^n (y_i - (wX_i + b))\n",
    "\\end{equation*}\n",
    "\n",
    "주어진 손실 함수와 계산된 gradient를 바탕으로, 아래 빈칸을 채워넣어 모델을 학습시키면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVoYaonjDdHd",
    "outputId": "d7edc302-de51-45e8-ee01-e810a2d0f266"
   },
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = 1000\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Model weights and bias parameters\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "\n",
    "# Store model parameters and loss for visualization\n",
    "w_list, b_list, loss_list = [], [], []\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(epochs):\n",
    "    \n",
    "    \n",
    "#################################################\n",
    "######## Hint: use +, -, *, /, **, np.mean() ########\n",
    "    loss = ???  # MSE loss\n",
    "    \n",
    "    dw = ???   # derivative w.r.t to w\n",
    "    db = ???   # derivative w.r.t to b\n",
    "#################################################\n",
    "    \n",
    "    \n",
    "    w = ???   # update w\n",
    "    b = ???   # update b\n",
    "    \n",
    "    w_list.append(w)\n",
    "    b_list.append(b)\n",
    "    loss_list.append(loss)\n",
    "\n",
    "print('Trained model weights : %.4f' % w)\n",
    "print('Trained model bias : %.4f' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cc8A8p9hDdHd"
   },
   "source": [
    "학습을 통해 다음의 것들을 시각화해보려 합니다.   \n",
    "1. 학습이 완료된 최적의 모델\n",
    "2. Epoch(에폭, 모델 학습의 반복 횟수)에 따른 학습된 모델\n",
    "3. Epoch에 따른 손실값 변화 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJAMg2T8DdHe",
    "outputId": "6c0f77ec-6f96-483c-fc20-d626e26f7bbf"
   },
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "y_pred = ???\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDX9OkflDdHe",
    "outputId": "d1dd052c-12c8-45e9-9c76-8d3fda9352da",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the intermediate trained model\n",
    "epochs_list = [1, 200, 400, 600, 800, 1000]\n",
    "\n",
    "for i in range(len(epochs_list)):\n",
    "    plt.scatter(X, y)   # scatter the original data\n",
    "    \n",
    "    # Load trained weights in specific epoch\n",
    "    epoch = epochs_list[i] - 1   # In python, all indexes start from 0\n",
    "    w = w_list[epoch]\n",
    "    b = b_list[epoch]\n",
    "    \n",
    "    y_pred = ???\n",
    "    plt.plot(X, y_pred, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jBSC0RgDdHf",
    "outputId": "3412c8ed-e57f-47ea-b356-7af9fe2d2436"
   },
   "outputs": [],
   "source": [
    "# Visualize the change of loss\n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qw1JRF6dDdHg"
   },
   "source": [
    "## [P2] 이번엔 `Pytorch` 패키지를 활용해 모델을 정의하고 경사하강법을 통해 학습하려 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 먼저, 데이터를 텐서 형태로 변환해야합니다.\n",
    "\n",
    "연산을 위해 X와 y를 텐서로 변환시켜주세요. 단, 텐서 타입 중 [float](https://pytorch.org/docs/stable/tensors.html) 타입으로 바꿔줄 것입니다.    \n",
    "그 다음, (400, 1)의 모양으로 변환시켜 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make X to float tensor type\n",
    "X_tensor = ???\n",
    "y_tensor = ???\n",
    "\n",
    "# reshpae as (400, 1)\n",
    "X_tensor = ???\n",
    "y_tensor = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형 회귀 모델을 정의해야 합니다.\n",
    "\n",
    "**(Hint 1)** 각 데이터 샘플마다 피처와 라벨의 차원이 몇 개일지 생각해봅시다.    \n",
    "**(Hint 2)** [Reference](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)를 참고해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ???\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델을 학습하기 위한 손실 함수를 정의해야 합니다.\n",
    "\n",
    "회귀 문제의 경우 [MSE 손실 함수](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)를, 분류 문제일 경우에는 [교차 엔트로피](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)를 손실 함수로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 또한, 경사하강법의 다양한 종류 중에서 어떤 것을 사용할지 결정해야 합니다.\n",
    "\n",
    "일반적으로 `optimizer`란 이름의 변수로, 이를 정의합니다.   \n",
    "옵티마이저는 `torch` 패키지 안의 [`optim`](https://pytorch.org/docs/stable/optim.html)이란 폴더에 구현이 되어 있습니다.    \n",
    "간단하게 SGD를 구현하고, 적절한 argument를 채워봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# define SGD\n",
    "optimizer = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습의 반복 횟수를 결정하는 하이퍼파라미터 `에폭(epoch)`은 python의 for문을 통해 구현합니다. \n",
    "\n",
    "하나의 에폭 안에서는 다음과 같이 모델 파라미터에 대한 경사하강법을 진행합니다.    \n",
    "다음의 [예제](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)를 통해, 아래 빈칸들을 채워보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for e in range(epochs):\n",
    "    # zero grad before the gradient descent\n",
    "    optimizer.???\n",
    "    \n",
    "    pred_y = ???\n",
    "    loss = ???\n",
    "    \n",
    "    # gradient descent\n",
    "    # calculate gradients\n",
    "    loss.???\n",
    "    \n",
    "    # update model with theire gradients\n",
    "    optimizer.???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXlpOVt0DdHg",
    "outputId": "90502ddf-0f43-4404-a344-0a69a4224a52"
   },
   "outputs": [],
   "source": [
    "# trained parameter value\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2irC2PHbDdHh"
   },
   "source": [
    "학습이 완료된 모델의 경우 다음과 같이 그려지며, `numpy` 패키지로 학습했던 결과와 동일하게 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxm4NBp1DdHh",
    "outputId": "334a799f-2b67-41d5-ad97-3dd68996d53a"
   },
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "\n",
    "# to plot by matplotlib, should transform output as numpy array type\n",
    "y_pred = ???\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teAmoTTZDdHi"
   },
   "source": [
    "이처럼, `Pytorch` 패키지를 활용하면 경사하강법을 단 몇 줄의 코드로 구현하여 쉽게 모델을 학습할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24pB9PdEDdHn"
   },
   "source": [
    "# 로지스틱 회귀 모델 (Classification 문제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHh1JThrDdHq",
    "outputId": "9c8eb77d-153f-4fe5-91b6-ca47cc3d3ac7"
   },
   "outputs": [],
   "source": [
    "with open('./data/logistic_regression.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data.head()   # show the 5 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpR2XDPwDdHq"
   },
   "source": [
    "### 로지스틱 회귀 모델을 이용해 유저의 `Age` 정보만을 가지고 구매를 했는지(1), 혹은 안했는지(0)를 학습해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRmKUmTEDdHr",
    "outputId": "0ea8f689-f640-4541-96da-89dec4e8280f"
   },
   "outputs": [],
   "source": [
    "# Extract 'Age' and 'Purchased' data\n",
    "X = data['Age'].to_numpy(dtype=np.float32)\n",
    "y = data['Purchased'].to_numpy()\n",
    "\n",
    "# Normalize 'Age' value\n",
    "def normalize(v):\n",
    "    v = v - v.mean()\n",
    "    return v\n",
    "\n",
    "X = normalize(X)\n",
    "\n",
    "# Visualizing the dataset\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3t_kR5a4DdHr"
   },
   "source": [
    "### 눈으로 데이터를 살펴봤을때, 구매를 한 사람들 중에서 나이가 많은 사람들이 많다는 경향성을 쉽게 파악할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WnA0xlnDdHr"
   },
   "source": [
    "## [P3] 이번에도 `Numpy` 패키지로만 로지스틱 회귀 모델을 학습해보겠습니다.\n",
    "\n",
    "로지스틱 회귀 모델은 0부터 1 사이의 값을 출력해주는 S 모양의 로지스틱 함수로 만들어졌습니다.   \n",
    "따라서 출력값은 확률의 의미를 지니고 있으며, 0.5의 출력값을 기준으로 0과 1의 이진 클래스로 분류해주는 모델이 됩니다.\n",
    "![logistic](./img/logistic_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 모델은 최대 가능도 추정(maximum likelihood estimation)을 기반으로 모델을 학습합니다.     \n",
    "어려운 개념이기에, 본 자료에서는 주어진 수식을 코드로 구현하는 연습만 해보려고 합니다.\n",
    "\n",
    "**Logistic regression model** (w와 b 파라미터 두가지)\n",
    "\\begin{equation*}\n",
    "P(y_i=1|X_i) = \\frac{1}{1 + e^{-(wX_i + b)}}\n",
    "\\end{equation*}\n",
    "\n",
    "**Loss function**\n",
    "\\begin{equation*}\n",
    "-\\frac{1}{n} \\sum_{i=1}^n (y_i * log(P(y_i=1|X_i)) + (1 - y_i) * log(1 - P(y_i=1|X_i)))\n",
    "\\end{equation*}\n",
    "\n",
    "**Gradients calculation**\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n ((P(y_i=1|X_i) - y_i) * X_i)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n (P(y_i=1|X_i) - y_i)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVKJTsgHDdHs",
    "outputId": "787d8f91-e17f-4cab-fa6c-396b6fba7246",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training setting\n",
    "epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Model weights and bias parameters\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "\n",
    "# Perform Gradient Descent\n",
    "\n",
    "for ??? # use for loop\n",
    "    \n",
    "    \n",
    "#################################################\n",
    "######## Hint: use +, -, *, /, **, np.mean(), #######\n",
    "########           np.exp(), np.log() ###########\n",
    "    y_pred = ???   # Logistic regression\n",
    "    loss = ???   # Likelihood loss\n",
    "    \n",
    "    dw = ???   # gradients w.r.t to w\n",
    "    db = ???   # gradients w.r.t to b\n",
    "    \n",
    "    # Update model weights and bias\n",
    "    w = ???\n",
    "    b = ???\n",
    "    \n",
    "#################################################\n",
    "\n",
    "\n",
    "print('Trained model weights : %.4f' % w)\n",
    "print('Trained model bias : %.4f' % b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3Sd-6IBDdHt"
   },
   "source": [
    "학습 결과를 시각화하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWq7h7FKDdHu",
    "outputId": "a12a0e7e-a7d6-4bad-9c99-9167dafc49e0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "y_pred = ???\n",
    "plt.scatter(X, y_pred, color='red')\n",
    "plt.axhline(y=0.5, color='orange', linestyle='--')   # show 0.5 threshold line\n",
    "\n",
    "# Accurate results are colored as orange\n",
    "index1 = (y == 0) * (y_pred < 0.5)\n",
    "index2 = (y == 1) * (y_pred > 0.5)\n",
    "index = index1 + index2\n",
    "plt.scatter(X[index], y[index], color='orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRCiTQhgDdHu"
   },
   "source": [
    "## [P4] `Pytorch` 패키지를 통해서 로지스틱 회귀 모델을 학습해보겠습니다.\n",
    "\n",
    "선형 회귀 모델을 통해 배웠던 순서대로 다음의 것들을 먼저 구현해보세요.\n",
    "1. 데이터를 텐서 형태로 변환\n",
    "2. 모델 정의\n",
    "3. 손실 함수 정의\n",
    "4. 옵티마이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data to tensor\n",
    "# make X and y to float tensor type\n",
    "X_tensor = ???\n",
    "y_tensor = ???\n",
    "\n",
    "# reshpae as (400, 1)\n",
    "X_tensor = ???\n",
    "y_tensor = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 경우, [`nn.Module`](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module)을 상속받은 새로운 로지스틱 회귀 모델에 대한 클래스를 만들어볼 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(???):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(???).??? # python inheritance\n",
    "        self.linear = ???  # hint: build FC layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = ???\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ???\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실 함수는 이진 클래스에 대한 교차 엔트로피인 [BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)로 설정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "옵티마이저는 이전과 동일하게 SGD를 사용해주시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정의한 변수들을 활용해, 경사하강법을 반복적으로 사용해 학습시켜 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "# make for loop\n",
    "???\n",
    "\n",
    "    # zero grad before the gradient descent\n",
    "    ???\n",
    "    \n",
    "    # get prediction output\n",
    "    ???\n",
    "    \n",
    "    # calculate loss\n",
    "    ???\n",
    "    \n",
    "    # gradient descent\n",
    "    # calculate gradients\n",
    "    ???\n",
    "    \n",
    "    # update model with theire gradients\n",
    "    ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkmp6mk0DdHv"
   },
   "source": [
    "학습된 결과를 보면, `Numpy` 패키지를 사용했을 때와 동일한 결과인 것을 알 수 있습니다.    \n",
    "이처럼 `pytorch` 패키지를 통해 경사하강법을 사용할 경우, 손실 함수에 대한 별도의 구현과 미분값 계산의 필요성이 사라집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClHplA8bDdHv",
    "outputId": "008c1d88-6cc4-4bc7-8162-860e8a9741a8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the trained linear regression model\n",
    "plt.scatter(X, y)   # scatter the original data\n",
    "\n",
    "# to plot by matplotlib, should transform output as numpy array type and 1 dimension\n",
    "y_pred = ???\n",
    "plt.scatter(X, y_pred, color='red')\n",
    "plt.axhline(y=0.5, color='orange', linestyle='--')   # show 0.5 threshold line\n",
    "\n",
    "# Accurate results are colored as orange\n",
    "index1 = (y == 0) * (y_pred < 0.5)\n",
    "index2 = (y == 1) * (y_pred > 0.5)\n",
    "index = index1 + index2\n",
    "plt.scatter(X[index], y[index.reshape(-1,)], color='orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zb1TJHK6DdHv"
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Understanding_backpropagation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
